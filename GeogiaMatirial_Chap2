1.Linear text classification
    文本分类问题用到的最朴素的模型是词袋模型（bag og words），实际上它在分类预测上准确率很高。
    bag of words 模型就是用词频表示文档，但要解决分类问题，即给定数据x(V向量)去预测y的值（K类），就要计算出x,与某个y同时出现时的分值， 
取分值最大的那个y即为预测结果。
    对多分类问题， 首先生成feature vector即f(x, y)，是k x V 的列向量, 对应y的位置为词频向量，其余位置由同等维度的0填充，得分
φ（x, y）= θ × f(x, y), 取值最大的y为预测分类的结果。其中x末尾需要加一个额外的feature(1), 其余的0向量也要拓展一维，所以f(x, y)
最终为k × (v + 1)维。
    对于某个特定的分类问题，如果人工设置好合适的θ（权重向量），　那么分类问题就解决了，但关键是人工设定这么高维度的θ不太现实，所以一般采用
监督机器学习方法学习出θ。
    1. Naive Bayes:
    p(x, y) = p(x|y) * p(y)　＝　p(y|x) * p(x)
    其中ｙ服从离散的概率分布，p(x|y)服从多元正态分布。
    在预测的新样本中，如果有新的单词，把它的概率设为０并不合适，因为这样ｐ(y|x)也变成０，这是不符合我们的期望的，所以给每个单词都加上一个常数，归一化。
    之所以叫Naive, 因为它将每个单词的出现视为相互独立，包括条件独立p(x1|y), p(x2|y)互相独立，但这不能很好的反应语言的实际特点即词语间的相关性。
    
    ２．Ｄiscreaminal Learning
    在优化θ*f(x, y)的过程中，可以根据结果进行权重调整，如果预测错误，则给θ增加正确标签的影响因子，否则θ不变。
    
